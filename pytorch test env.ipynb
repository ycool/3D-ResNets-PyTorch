{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n",
      "3.7.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.module import _addindent\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, lr_scheduler\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.backends import cudnn\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchviz import make_dot\n",
    "from graphviz import Source\n",
    "\n",
    "from opts import parse_opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hujiangtao/miniconda2/lib/python3.7/site-packages/scipy/__init__.py:137: UserWarning: NumPy 1.16.5 or above is required for this version of SciPy (detected version 1.16.4)\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from model import (generate_model, load_pretrained_model, make_data_parallel,\n",
    "                   get_fine_tuning_parameters)\n",
    "from mean import get_mean_std\n",
    "from spatial_transforms import (Compose, Normalize, Resize, CenterCrop,\n",
    "                                CornerCrop, MultiScaleCornerCrop,\n",
    "                                RandomResizedCrop, RandomHorizontalFlip,\n",
    "                                ToTensor, ScaleValue, ColorJitter,\n",
    "                                PickFirstChannels)\n",
    "from temporal_transforms import (LoopPadding, TemporalRandomCrop,\n",
    "                                 TemporalCenterCrop, TemporalEvenCrop,\n",
    "                                 SlidingWindow, TemporalSubsampling)\n",
    "from temporal_transforms import Compose as TemporalCompose\n",
    "from dataset import get_training_data, get_validation_data, get_inference_data\n",
    "from utils import Logger, worker_init_fn, get_lr\n",
    "from training import train_epoch\n",
    "from validation import val_epoch\n",
    "import inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_serial(obj):\n",
    "    if isinstance(obj, Path):\n",
    "        return str(obj)\n",
    "\n",
    "\n",
    "def get_opt():\n",
    "    opt = parse_opts()\n",
    "\n",
    "    if opt.root_path is not None:\n",
    "        opt.video_path = opt.root_path / opt.video_path\n",
    "        opt.annotation_path = opt.root_path / opt.annotation_path\n",
    "        opt.result_path = opt.root_path / opt.result_path\n",
    "        if opt.resume_path is not None:\n",
    "            opt.resume_path = opt.root_path / opt.resume_path\n",
    "        if opt.pretrain_path is not None:\n",
    "            opt.pretrain_path = opt.root_path / opt.pretrain_path\n",
    "\n",
    "    if opt.pretrain_path is not None:\n",
    "        opt.n_finetune_classes = opt.n_classes\n",
    "        opt.n_classes = opt.n_pretrain_classes\n",
    "\n",
    "    if opt.output_topk <= 0:\n",
    "        opt.output_topk = opt.n_classes\n",
    "\n",
    "    if opt.inference_batch_size == 0:\n",
    "        opt.inference_batch_size = opt.batch_size\n",
    "\n",
    "    opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
    "    opt.begin_epoch = 1\n",
    "    opt.mean, opt.std = get_mean_std(opt.value_scale, dataset=opt.mean_dataset)\n",
    "    opt.n_input_channels = 3\n",
    "    if opt.input_type == 'flow':\n",
    "        opt.n_input_channels = 2\n",
    "        opt.mean = opt.mean[:2]\n",
    "        opt.std = opt.std[:2]\n",
    "\n",
    "    if opt.distributed:\n",
    "        opt.dist_rank = int(os.environ[\"OMPI_COMM_WORLD_RANK\"])\n",
    "\n",
    "        if opt.dist_rank == 0:\n",
    "            print(opt)\n",
    "            with (opt.result_path / 'opts.json').open('w') as opt_file:\n",
    "                json.dump(vars(opt), opt_file, default=json_serial)\n",
    "    else:\n",
    "        print(opt)\n",
    "        with (opt.result_path / 'opts.json').open('w') as opt_file:\n",
    "            json.dump(vars(opt), opt_file, default=json_serial)\n",
    "\n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_model(resume_path, arch, model):\n",
    "    print('loading checkpoint {} model'.format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location='cpu')\n",
    "    assert arch == checkpoint['arch']\n",
    "\n",
    "    if hasattr(model, 'module'):\n",
    "        model.module.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def resume_train_utils(resume_path, begin_epoch, optimizer, scheduler):\n",
    "    print('loading checkpoint {} train utils'.format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location='cpu')\n",
    "\n",
    "    begin_epoch = checkpoint['epoch'] + 1\n",
    "    if optimizer is not None and 'optimizer' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    if scheduler is not None and 'scheduler' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    return begin_epoch, optimizer, scheduler\n",
    "\n",
    "def get_normalize_method(mean, std, no_mean_norm, no_std_norm):\n",
    "    if no_mean_norm:\n",
    "        if no_std_norm:\n",
    "            return Normalize([0, 0, 0], [1, 1, 1])\n",
    "        else:\n",
    "            return Normalize([0, 0, 0], std)\n",
    "    else:\n",
    "        if no_std_norm:\n",
    "            return Normalize(mean, [1, 1, 1])\n",
    "        else:\n",
    "            return Normalize(mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_utils(opt, model_parameters):\n",
    "    assert opt.train_crop in ['random', 'corner', 'center']\n",
    "    spatial_transform = []\n",
    "    if opt.train_crop == 'random':\n",
    "        spatial_transform.append(\n",
    "            RandomResizedCrop(\n",
    "                opt.sample_size, (opt.train_crop_min_scale, 1.0),\n",
    "                (opt.train_crop_min_ratio, 1.0 / opt.train_crop_min_ratio)))\n",
    "    elif opt.train_crop == 'corner':\n",
    "        scales = [1.0]\n",
    "        scale_step = 1 / (2**(1 / 4))\n",
    "        for _ in range(1, 5):\n",
    "            scales.append(scales[-1] * scale_step)\n",
    "        spatial_transform.append(MultiScaleCornerCrop(opt.sample_size, scales))\n",
    "    elif opt.train_crop == 'center':\n",
    "        spatial_transform.append(Resize(opt.sample_size))\n",
    "        spatial_transform.append(CenterCrop(opt.sample_size))\n",
    "    normalize = get_normalize_method(opt.mean, opt.std, opt.no_mean_norm,\n",
    "                                     opt.no_std_norm)\n",
    "    if not opt.no_hflip:\n",
    "        spatial_transform.append(RandomHorizontalFlip())\n",
    "    if opt.colorjitter:\n",
    "        spatial_transform.append(ColorJitter())\n",
    "    spatial_transform.append(ToTensor())\n",
    "    if opt.input_type == 'flow':\n",
    "        spatial_transform.append(PickFirstChannels(n=2))\n",
    "    spatial_transform.append(ScaleValue(opt.value_scale))\n",
    "    spatial_transform.append(normalize)\n",
    "    spatial_transform = Compose(spatial_transform)\n",
    "\n",
    "    assert opt.train_t_crop in ['random', 'center']\n",
    "    temporal_transform = []\n",
    "    if opt.sample_t_stride > 1:\n",
    "        temporal_transform.append(TemporalSubsampling(opt.sample_t_stride))\n",
    "    if opt.train_t_crop == 'random':\n",
    "        temporal_transform.append(TemporalRandomCrop(opt.sample_duration))\n",
    "    elif opt.train_t_crop == 'center':\n",
    "        temporal_transform.append(TemporalCenterCrop(opt.sample_duration))\n",
    "    temporal_transform = TemporalCompose(temporal_transform)\n",
    "\n",
    "    train_data = get_training_data(opt.video_path, opt.annotation_path,\n",
    "                                   opt.dataset, opt.input_type, opt.file_type,\n",
    "                                   spatial_transform, temporal_transform)\n",
    "    if opt.distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            train_data)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=opt.batch_size,\n",
    "                                               shuffle=(train_sampler is None),\n",
    "                                               num_workers=opt.n_threads,\n",
    "                                               pin_memory=True,\n",
    "                                               sampler=train_sampler,\n",
    "                                               worker_init_fn=worker_init_fn)\n",
    "\n",
    "    if opt.is_master_node:\n",
    "        train_logger = Logger(opt.result_path / 'train.log',\n",
    "                              ['epoch', 'loss', 'acc', 'lr'])\n",
    "        train_batch_logger = Logger(\n",
    "            opt.result_path / 'train_batch.log',\n",
    "            ['epoch', 'batch', 'iter', 'loss', 'acc', 'lr'])\n",
    "    else:\n",
    "        train_logger = None\n",
    "        train_batch_logger = None\n",
    "\n",
    "    if opt.nesterov:\n",
    "        dampening = 0\n",
    "    else:\n",
    "        dampening = opt.dampening\n",
    "    optimizer = SGD(model_parameters,\n",
    "                    lr=opt.learning_rate,\n",
    "                    momentum=opt.momentum,\n",
    "                    dampening=dampening,\n",
    "                    weight_decay=opt.weight_decay,\n",
    "                    nesterov=opt.nesterov)\n",
    "\n",
    "    assert opt.lr_scheduler in ['plateau', 'multistep']\n",
    "    assert not (opt.lr_scheduler == 'plateau' and opt.no_val)\n",
    "    if opt.lr_scheduler == 'plateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, 'min', patience=opt.plateau_patience)\n",
    "    else:\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer,\n",
    "                                             opt.multistep_milestones)\n",
    "\n",
    "    return (train_loader, train_sampler, train_logger, train_batch_logger,\n",
    "            optimizer, scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_utils(opt):\n",
    "    normalize = get_normalize_method(opt.mean, opt.std, opt.no_mean_norm,\n",
    "                                     opt.no_std_norm)\n",
    "    spatial_transform = [\n",
    "        Resize(opt.sample_size),\n",
    "        CenterCrop(opt.sample_size),\n",
    "        ToTensor()\n",
    "    ]\n",
    "    if opt.input_type == 'flow':\n",
    "        spatial_transform.append(PickFirstChannels(n=2))\n",
    "    spatial_transform.extend([ScaleValue(opt.value_scale), normalize])\n",
    "    spatial_transform = Compose(spatial_transform)\n",
    "\n",
    "    temporal_transform = []\n",
    "    if opt.sample_t_stride > 1:\n",
    "        temporal_transform.append(TemporalSubsampling(opt.sample_t_stride))\n",
    "    temporal_transform.append(\n",
    "        TemporalEvenCrop(opt.sample_duration, opt.n_val_samples))\n",
    "    temporal_transform = TemporalCompose(temporal_transform)\n",
    "\n",
    "    val_data, collate_fn = get_validation_data(opt.video_path,\n",
    "                                               opt.annotation_path, opt.dataset,\n",
    "                                               opt.input_type, opt.file_type,\n",
    "                                               spatial_transform,\n",
    "                                               temporal_transform)\n",
    "    if opt.distributed:\n",
    "        val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            val_data, shuffle=False)\n",
    "    else:\n",
    "        val_sampler = None\n",
    "    val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                             batch_size=(opt.batch_size //\n",
    "                                                         opt.n_val_samples),\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=opt.n_threads,\n",
    "                                             pin_memory=True,\n",
    "                                             sampler=val_sampler,\n",
    "                                             worker_init_fn=worker_init_fn,\n",
    "                                             collate_fn=collate_fn)\n",
    "\n",
    "    if opt.is_master_node:\n",
    "        val_logger = Logger(opt.result_path / 'val.log',\n",
    "                            ['epoch', 'loss', 'acc'])\n",
    "    else:\n",
    "        val_logger = None\n",
    "\n",
    "    return val_loader, val_logger\n",
    "\n",
    "\n",
    "def get_inference_utils(opt):\n",
    "    assert opt.inference_crop in ['center', 'nocrop']\n",
    "\n",
    "    normalize = get_normalize_method(opt.mean, opt.std, opt.no_mean_norm,\n",
    "                                     opt.no_std_norm)\n",
    "\n",
    "    spatial_transform = [Resize(opt.sample_size)]\n",
    "    if opt.inference_crop == 'center':\n",
    "        spatial_transform.append(CenterCrop(opt.sample_size))\n",
    "    spatial_transform.append(ToTensor())\n",
    "    if opt.input_type == 'flow':\n",
    "        spatial_transform.append(PickFirstChannels(n=2))\n",
    "    spatial_transform.extend([ScaleValue(opt.value_scale), normalize])\n",
    "    spatial_transform = Compose(spatial_transform)\n",
    "\n",
    "    temporal_transform = []\n",
    "    if opt.sample_t_stride > 1:\n",
    "        temporal_transform.append(TemporalSubsampling(opt.sample_t_stride))\n",
    "    temporal_transform.append(\n",
    "        SlidingWindow(opt.sample_duration, opt.inference_stride))\n",
    "    temporal_transform = TemporalCompose(temporal_transform)\n",
    "\n",
    "    inference_data, collate_fn = get_inference_data(\n",
    "        opt.video_path, opt.annotation_path, opt.dataset, opt.input_type,\n",
    "        opt.file_type, opt.inference_subset, spatial_transform,\n",
    "        temporal_transform)\n",
    "\n",
    "    inference_loader = torch.utils.data.DataLoader(\n",
    "        inference_data,\n",
    "        batch_size=opt.inference_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=opt.n_threads,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    return inference_loader, inference_data.class_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(save_file_path, epoch, arch, model, optimizer, scheduler):\n",
    "    if hasattr(model, 'module'):\n",
    "        model_state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        model_state_dict = model.state_dict()\n",
    "    save_states = {\n",
    "        'epoch': epoch,\n",
    "        'arch': arch,\n",
    "        'state_dict': model_state_dict,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(save_states, save_file_path)\n",
    "\n",
    "\n",
    "def torch_summarize(model, show_weights=True, show_parameters=True):\n",
    "    \"\"\"Summarizes torch model by showing trainable parameters and weights.\"\"\"\n",
    "    tmpstr = model.__class__.__name__ + ' (\\n'\n",
    "    num_layer = 0\n",
    "    for key, module in model._modules.items():\n",
    "        # if it contains layers let call it recursively to get params and weights\n",
    "        if type(module) in [\n",
    "            torch.nn.modules.container.Container,\n",
    "            torch.nn.modules.container.Sequential\n",
    "        ]:\n",
    "            modstr = torch_summarize(module)\n",
    "        else:\n",
    "            modstr = module.__repr__()\n",
    "        modstr = _addindent(modstr, 2)\n",
    "\n",
    "        params = sum([np.prod(p.size()) for p in module.parameters()])\n",
    "        weights = tuple([tuple(p.size()) for p in module.parameters()])\n",
    "\n",
    "        tmpstr += '  (' + key + '): ' + modstr \n",
    "        if show_weights:\n",
    "            tmpstr += ', weights={}'.format(weights)\n",
    "        if show_parameters:\n",
    "            tmpstr +=  ', parameters={}'.format(params)\n",
    "        tmpstr += '\\n'\n",
    "        num_layer += 1\n",
    "\n",
    "    tmpstr = tmpstr + ')'\n",
    "    tmpstr = tmpstr + ' num of layer:' + str(num_layer)\n",
    "    return tmpstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(index, opt):\n",
    "    random.seed(opt.manual_seed)\n",
    "    np.random.seed(opt.manual_seed)\n",
    "    torch.manual_seed(opt.manual_seed)\n",
    "\n",
    "    if index >= 0 and opt.device.type == 'cuda':\n",
    "        opt.device = torch.device(f'cuda:{index}')\n",
    "\n",
    "    opt.is_master_node = not opt.distributed or opt.dist_rank == 0\n",
    "\n",
    "    model = generate_model(opt)\n",
    "    print('after generating model:', model.fc.in_features,':',  model.fc.out_features)\n",
    "    print('feature weights:', model.fc.weight.shape,':',  model.fc.bias.shape)\n",
    "\n",
    "    if opt.resume_path is not None:\n",
    "        model = resume_model(opt.resume_path, opt.arch, model)\n",
    "    print('after resume model:', model.fc.in_features,':',  model.fc.out_features)\n",
    "    print('feature weights:', model.fc.weight.shape,':',  model.fc.bias.shape)\n",
    "    # summary(model, input_size=(3, 112, 112))\n",
    "#    if opt.pretrain_path:\n",
    "#        model = load_pretrained_model(model, opt.pretrain_path, opt.model,\n",
    "#                                      opt.n_finetune_classes)\n",
    "\n",
    "    print('after pretrained  model:', model.fc.in_features,':',  model.fc.out_features)\n",
    "    print('feature weights:', model.fc.weight.shape,':',  model.fc.bias.shape)\n",
    "    print(torch_summarize(model))\n",
    "    # parameters = model.parameters()\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name, param.data)\n",
    "#    summary(model, (3, 112, 112))\n",
    "#    return\n",
    "\n",
    "#    print('model parameters shape', parameters.shape)\n",
    "\n",
    "    (train_loader, train_sampler, train_logger, train_batch_logger,\n",
    "     optimizer, scheduler) = get_train_utils(opt, model.parameters())\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        print('input shape:', inputs.shape)\n",
    "        print('targets shape:', targets.shape)\n",
    "        outputs = model(inputs)\n",
    "        print(\"output shape\", outputs.shape)\n",
    "        model_arch = make_dot(outputs, params=dict(model.named_parameters()))\n",
    "        print(model_arch)\n",
    "        model_arch.render(\"/apollo/data/model.png\", format=\"png\")\n",
    "        # Source(model_arch).render('/apollo/data/model.png')\n",
    "        # print(\"generating /apollo/data/model.png\")\n",
    "        break\n",
    "    \n",
    "    # make_dot(yhat, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "    if opt.batchnorm_sync:\n",
    "        assert opt.distributed, 'SyncBatchNorm only supports DistributedDataParallel.'\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "    if opt.pretrain_path:\n",
    "        model = load_pretrained_model(model, opt.pretrain_path, opt.model,\n",
    "                                      opt.n_finetune_classes)\n",
    "    if opt.resume_path is not None:\n",
    "        model = resume_model(opt.resume_path, opt.arch, model)\n",
    "    model = make_data_parallel(model, opt.distributed, opt.device)\n",
    "\n",
    "    if opt.pretrain_path:\n",
    "        parameters = get_fine_tuning_parameters(model, opt.ft_begin_module)\n",
    "    else:\n",
    "        parameters = model.parameters()\n",
    "\n",
    "    if opt.is_master_node:\n",
    "        print(model)\n",
    "\n",
    "    criterion = CrossEntropyLoss().to(opt.device)\n",
    "\n",
    "    if not opt.no_train:\n",
    "        (train_loader, train_sampler, train_logger, train_batch_logger,\n",
    "         optimizer, scheduler) = get_train_utils(opt, parameters)\n",
    "        if opt.resume_path is not None:\n",
    "            opt.begin_epoch, optimizer, scheduler = resume_train_utils(\n",
    "                opt.resume_path, opt.begin_epoch, optimizer, scheduler)\n",
    "            if opt.overwrite_milestones:\n",
    "                scheduler.milestones = opt.multistep_milestones\n",
    "    if not opt.no_val:\n",
    "        val_loader, val_logger = get_val_utils(opt)\n",
    "\n",
    "    if opt.tensorboard and opt.is_master_node:\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        if opt.begin_epoch == 1:\n",
    "            tb_writer = SummaryWriter(log_dir=opt.result_path)\n",
    "        else:\n",
    "            tb_writer = SummaryWriter(log_dir=opt.result_path,\n",
    "                                      purge_step=opt.begin_epoch)\n",
    "    else:\n",
    "        tb_writer = None\n",
    "\n",
    "    prev_val_loss = None\n",
    "    for i in range(opt.begin_epoch, opt.n_epochs + 1):\n",
    "        if not opt.no_train:\n",
    "            if opt.distributed:\n",
    "                train_sampler.set_epoch(i)\n",
    "            current_lr = get_lr(optimizer)\n",
    "            train_epoch(i, train_loader, model, criterion, optimizer,\n",
    "                        opt.device, current_lr, train_logger,\n",
    "                        train_batch_logger, tb_writer, opt.distributed)\n",
    "\n",
    "            if i % opt.checkpoint == 0 and opt.is_master_node:\n",
    "                save_file_path = opt.result_path / 'save_{}.pth'.format(i)\n",
    "                save_checkpoint(save_file_path, i, opt.arch, model, optimizer,\n",
    "                                scheduler)\n",
    "\n",
    "        if not opt.no_val:\n",
    "            prev_val_loss = val_epoch(i, val_loader, model, criterion,\n",
    "                                      opt.device, val_logger, tb_writer,\n",
    "                                      opt.distributed)\n",
    "\n",
    "        if not opt.no_train and opt.lr_scheduler == 'multistep':\n",
    "            scheduler.step()\n",
    "        elif not opt.no_train and opt.lr_scheduler == 'plateau':\n",
    "            scheduler.step(prev_val_loss)\n",
    "\n",
    "    if opt.inference:\n",
    "        inference_loader, inference_class_names = get_inference_utils(opt)\n",
    "        inference_result_path = opt.result_path / '{}.json'.format(\n",
    "            opt.inference_subset)\n",
    "\n",
    "        inference.inference(inference_loader, model, inference_result_path,\n",
    "                            inference_class_names, opt.inference_no_average,\n",
    "                            opt.output_topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(accimage=False, annotation_path=None, arch='resnet-18', batch_size=128, batchnorm_sync=False, begin_epoch=1, checkpoint=10, colorjitter=False, conv1_t_size=7, conv1_t_stride=1, dampening=0.0, dataset='kinetics', dist_url='tcp://127.0.0.1:23456', distributed=False, fff='/Users/hujiangtao/Library/Jupyter/runtime/kernel-debf04db-7e2c-4a1d-8816-a6417ea94215.json', file_type='jpg', ft_begin_module='', inference=False, inference_batch_size=128, inference_crop='center', inference_no_average=False, inference_stride=16, inference_subset='val', input_type='rgb', learning_rate=0.1, lr_scheduler='multistep', manual_seed=1, mean=[0.4345, 0.4051, 0.3775], mean_dataset='kinetics', model='resnet', model_depth=18, momentum=0.9, multistep_milestones=[50, 100, 150], n_classes=400, n_epochs=200, n_input_channels=3, n_pretrain_classes=0, n_threads=4, n_val_samples=3, nesterov=False, no_cuda=False, no_hflip=False, no_max_pool=False, no_mean_norm=False, no_std_norm=False, no_train=False, no_val=False, optimizer='sgd', output_topk=5, overwrite_milestones=False, plateau_patience=10, pretrain_path=None, resnet_shortcut='B', resnet_widen_factor=1.0, resnext_cardinality=32, result_path=PosixPath('results'), resume_path=None, root_path=None, sample_duration=16, sample_size=112, sample_t_stride=1, std=[0.2768, 0.2713, 0.2737], tensorboard=False, train_crop='random', train_crop_min_ratio=0.75, train_crop_min_scale=0.25, train_t_crop='random', value_scale=1, video_path=None, weight_decay=0.001, wide_resnet_k=2, world_size=-1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "opt = get_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after generating model: 512 : 400\n",
      "feature weights: torch.Size([400, 512]) : torch.Size([400])\n",
      "after resume model: 512 : 400\n",
      "feature weights: torch.Size([400, 512]) : torch.Size([400])\n",
      "after pretrained  model: 512 : 400\n",
      "feature weights: torch.Size([400, 512]) : torch.Size([400])\n",
      "ResNet (\n",
      "  (conv1): Conv3d(3, 64, kernel_size=(7, 7, 7), stride=(1, 2, 2), padding=(3, 3, 3), bias=False), weights=((64, 3, 7, 7, 7),), parameters=65856\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), weights=((64,), (64,)), parameters=128\n",
      "  (relu): ReLU(inplace=True), weights=(), parameters=0\n",
      "  (maxpool): MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False), weights=(), parameters=0\n",
      "  (layer1): Sequential (\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    ), weights=((64, 64, 3, 3, 3), (64,), (64,), (64, 64, 3, 3, 3), (64,), (64,)), parameters=221440\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    ), weights=((64, 64, 3, 3, 3), (64,), (64,), (64, 64, 3, 3, 3), (64,), (64,)), parameters=221440\n",
      "  ) num of layer:2, weights=((64, 64, 3, 3, 3), (64,), (64,), (64, 64, 3, 3, 3), (64,), (64,), (64, 64, 3, 3, 3), (64,), (64,), (64, 64, 3, 3, 3), (64,), (64,)), parameters=442880\n",
      "  (layer2): Sequential (\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    ), weights=((128, 64, 3, 3, 3), (128,), (128,), (128, 128, 3, 3, 3), (128,), (128,), (128, 64, 1, 1, 1), (128,), (128,)), parameters=672512\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    ), weights=((128, 128, 3, 3, 3), (128,), (128,), (128, 128, 3, 3, 3), (128,), (128,)), parameters=885248\n",
      "  ) num of layer:2, weights=((128, 64, 3, 3, 3), (128,), (128,), (128, 128, 3, 3, 3), (128,), (128,), (128, 64, 1, 1, 1), (128,), (128,), (128, 128, 3, 3, 3), (128,), (128,), (128, 128, 3, 3, 3), (128,), (128,)), parameters=1557760\n",
      "  (layer3): Sequential (\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    ), weights=((256, 128, 3, 3, 3), (256,), (256,), (256, 256, 3, 3, 3), (256,), (256,), (256, 128, 1, 1, 1), (256,), (256,)), parameters=2688512\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    ), weights=((256, 256, 3, 3, 3), (256,), (256,), (256, 256, 3, 3, 3), (256,), (256,)), parameters=3539968\n",
      "  ) num of layer:2, weights=((256, 128, 3, 3, 3), (256,), (256,), (256, 256, 3, 3, 3), (256,), (256,), (256, 128, 1, 1, 1), (256,), (256,), (256, 256, 3, 3, 3), (256,), (256,), (256, 256, 3, 3, 3), (256,), (256,)), parameters=6228480\n",
      "  (layer4): Sequential (\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    ), weights=((512, 256, 3, 3, 3), (512,), (512,), (512, 512, 3, 3, 3), (512,), (512,), (512, 256, 1, 1, 1), (512,), (512,)), parameters=10750976\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    ), weights=((512, 512, 3, 3, 3), (512,), (512,), (512, 512, 3, 3, 3), (512,), (512,)), parameters=14157824\n",
      "  ) num of layer:2, weights=((512, 256, 3, 3, 3), (512,), (512,), (512, 512, 3, 3, 3), (512,), (512,), (512, 256, 1, 1, 1), (512,), (512,), (512, 512, 3, 3, 3), (512,), (512,), (512, 512, 3, 3, 3), (512,), (512,)), parameters=24908800\n",
      "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1)), weights=(), parameters=0\n",
      "  (fc): Linear(in_features=512, out_features=400, bias=True), weights=((400, 512), (400,)), parameters=205200\n",
      ") num of layer:10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'open'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-76de7f9b920b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngpus_per_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmain_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-c3d84f8f8d7a>\u001b[0m in \u001b[0;36mmain_worker\u001b[0;34m(index, opt)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     (train_loader, train_sampler, train_logger, train_batch_logger,\n\u001b[0;32m---> 37\u001b[0;31m      optimizer, scheduler) = get_train_utils(opt, model.parameters())\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-55c3ac8386f0>\u001b[0m in \u001b[0;36mget_train_utils\u001b[0;34m(opt, model_parameters)\u001b[0m\n\u001b[1;32m     41\u001b[0m     train_data = get_training_data(opt.video_path, opt.annotation_path,\n\u001b[1;32m     42\u001b[0m                                    \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                    spatial_transform, temporal_transform)\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
      "\u001b[0;32m~/action/3D-ResNets-PyTorch/dataset.py\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(video_path, annotation_path, dataset_name, input_type, file_type, spatial_transform, temporal_transform, target_transform)\u001b[0m\n\u001b[1;32m     62\u001b[0m                                      \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                      \u001b[0mvideo_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                                      video_path_formatter=video_path_formatter)\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/action/3D-ResNets-PyTorch/datasets/videodataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_path, annotation_path, subset, spatial_transform, temporal_transform, target_transform, video_loader, video_path_formatter, image_name_formatter, target_type)\u001b[0m\n\u001b[1;32m     51\u001b[0m                  target_type='label'):\n\u001b[1;32m     52\u001b[0m         self.data, self.class_names = self.__make_dataset(\n\u001b[0;32m---> 53\u001b[0;31m             root_path, annotation_path, subset, video_path_formatter)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/action/3D-ResNets-PyTorch/datasets/videodataset.py\u001b[0m in \u001b[0;36m__make_dataset\u001b[0;34m(self, root_path, annotation_path, subset, video_path_formatter)\u001b[0m\n\u001b[1;32m     66\u001b[0m     def __make_dataset(self, root_path, annotation_path, subset,\n\u001b[1;32m     67\u001b[0m                        video_path_formatter):\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mannotation_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         video_ids, video_paths, annotations = get_database(\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'open'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "opt.device = torch.device('cpu' if opt.no_cuda else 'cuda')\n",
    "if not opt.no_cuda:\n",
    "    cudnn.benchmark = True\n",
    "if opt.accimage:\n",
    "    torchvision.set_image_backend('accimage')\n",
    "\n",
    "opt.ngpus_per_node = torch.cuda.device_count()\n",
    "main_worker(-1, opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
